{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a5501",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from google.colab import files\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Heart Failure Prediction System\")\n",
    "\n",
    "# Upload Dataset 1 (299 rows) heart_failure_clinical_records_dataset\n",
    "print(\"First, please upload 'heart_failure_clinical_records_dataset.csv'\")\n",
    "uploaded1 = files.upload()\n",
    "file_name1 = list(uploaded1.keys())[0]\n",
    "data1 = pd.read_csv(io.BytesIO(uploaded1[file_name1]))\n",
    "print(f\"Successfully uploaded {file_name1}\\n\")\n",
    "\n",
    "# Upload Dataset 2 (918 rows) heart\n",
    "print(\"Second, please upload 'heart.csv'\")\n",
    "uploaded2 = files.upload()\n",
    "file_name2 = list(uploaded2.keys())[0]\n",
    "data2 = pd.read_csv(io.BytesIO(uploaded2[file_name2]))\n",
    "print(f\"Successfully uploaded {file_name2}\\n\")\n",
    "\n",
    "\n",
    "# Preprocessing and Evaluating first dataset\n",
    "\n",
    "print(\"PART 1: Project 'DEATH_EVENT' (299-row Dataset)\")\n",
    "\n",
    "\n",
    "# Data Exploration\n",
    "print(\"Statistical Summary (Dataset 1)\")\n",
    "print(data1.describe())\n",
    "\n",
    "# Preprocessing\n",
    "print(\"\\nPreprocessing (Dataset 1)\")\n",
    "# This dataset is all numerical, so we only need to scale it.\n",
    "X1 = data1.drop('DEATH_EVENT', axis=1)\n",
    "y1 = data1['DEATH_EVENT']\n",
    "\n",
    "# 80/20 split for training and testing\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "print(f\"Dataset 1: {X1_train.shape[0]} training samples, {X1_test.shape[0]} testing samples.\")\n",
    "\n",
    "# Scale the data\n",
    "scaler1 = StandardScaler()\n",
    "X1_train_scaled = scaler1.fit_transform(X1_train)\n",
    "X1_test_scaled = scaler1.transform(X1_test)\n",
    "print(\"Dataset 1 preprocessed and scaled.\")\n",
    "\n",
    "# Defining Models\n",
    "models_1 = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Support Vector (SVC)\": SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Training and Evaluating All Models\n",
    "print(\"\\nModel Training & Evaluation (Dataset 1)\")\n",
    "\n",
    "for name, model in models_1.items():\n",
    "    print(f\"\\n--- Results for: {name} ---\")\n",
    "\n",
    "    # Training\n",
    "    model.fit(X1_train_scaled, y1_train)\n",
    "\n",
    "    # Predicting\n",
    "    y_pred1 = model.predict(X1_test_scaled)\n",
    "\n",
    "    # Evaluating\n",
    "    accuracy = accuracy_score(y1_test, y_pred1)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y1_test, y_pred1, target_names=['Did Not Die (0)', 'Died (1)']))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm1 = confusion_matrix(y1_test, y_pred1)\n",
    "    print(cm1)\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm1, annot=True, fmt='g', cmap='Blues',\n",
    "                xticklabels=['Predicted: No Death', 'Predicted: Death'],\n",
    "                yticklabels=['Actual: No Death', 'Actual: Death'])\n",
    "    plt.title(f\"Confusion Matrix for {name} (Dataset 1)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Preprocessing and Evaluating first dataset\n",
    "\n",
    "print(\"PART 2: Project 'HeartDisease' (918-row Dataset)\")\n",
    "\n",
    "# Data Exploration\n",
    "print(\"\\nStatistical Summary (Dataset 2)\")\n",
    "print(data2.describe())\n",
    "\n",
    "# Preprocessing\n",
    "print(\"\\nPreprocessing (Dataset 2)\")\n",
    "# This dataset has mixed (numerical and categorical) data\n",
    "X2 = data2.drop('HeartDisease', axis=1)\n",
    "y2 = data2['HeartDisease']\n",
    "\n",
    "# Identify column types\n",
    "numeric_features = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak']\n",
    "categorical_features = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "\n",
    "# Create the preprocessor\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# 80/20 split for training and testing\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "print(f\"Dataset 2: {X2_train.shape[0]} training samples, {X2_test.shape[0]} testing samples.\")\n",
    "\n",
    "# Apply the preprocessor\n",
    "X2_train_processed = preprocessor.fit_transform(X2_train)\n",
    "X2_test_processed = preprocessor.transform(X2_test)\n",
    "print(\"Dataset 2 preprocessed, encoded, and scaled.\")\n",
    "\n",
    "# Define Models\n",
    "models_2 = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Support Vector (SVC)\": SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Training and Evaluating All Models\n",
    "print(\"\\nModel Training & Evaluation (Dataset 2)\")\n",
    "\n",
    "for name, model in models_2.items():\n",
    "    print(f\"\\nResults for: {name}\")\n",
    "\n",
    "    # Training\n",
    "    model.fit(X2_train_processed, y2_train)\n",
    "\n",
    "    # Predicting\n",
    "    y_pred2 = model.predict(X2_test_processed)\n",
    "\n",
    "    # Evaluating\n",
    "    accuracy = accuracy_score(y2_test, y_pred2)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y2_test, y_pred2, target_names=['No Disease (0)', 'Has Disease (1)']))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm2 = confusion_matrix(y2_test, y_pred2)\n",
    "    print(cm2)\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm2, annot=True, fmt='g', cmap='Greens',\n",
    "                xticklabels=['Predicted: No Disease', 'Predicted: Disease'],\n",
    "                yticklabels=['Actual: No Disease', 'Actual: Disease'])\n",
    "    plt.title(f\"Confusion Matrix for {name} (Dataset 2)\")\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
